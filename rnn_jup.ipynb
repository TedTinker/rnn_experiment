{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5f97d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Alice in Wonderland: \t143426 characters.\n",
      "Different characters: \t\t38 characters.\n",
      "There are 143421 sequences of 5 characters in a row.\n",
      "\n",
      "Train: \n",
      "\t Input: (121907, 5, 38), \t Output: (121907, 38).\n",
      "\n",
      "Test: \n",
      "\t Input: (21514, 5, 38), \t Output: (21514, 38).\n"
     ]
    }
   ],
   "source": [
    "### First get data.\n",
    "\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical # Just for one-hots!\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\tedjt\\Desktop\\OIST\\A313\")\n",
    "raw_text = open(\"wonderland.txt\", 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower().replace(\"\\n\", \" \")\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars, n_vocab = len(raw_text), len(chars)\n",
    "seq_length = 5\n",
    "T = seq_length\n",
    "data_x, data_y = [], []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    data_x.append([char_to_int[char] for char in seq_in])\n",
    "    data_y.append(char_to_int[seq_out])\n",
    "n_patterns = len(data_x)\n",
    "\n",
    "x = np.reshape(data_x, (n_patterns, seq_length))\n",
    "x = to_categorical(data_x)\n",
    "y = to_categorical(data_y)\n",
    "x_keep, y_keep = x, y\n",
    "\n",
    "print(\"Length of Alice in Wonderland: \\t{} characters.\".format(n_chars))\n",
    "print(\"Different characters: \\t\\t{} characters.\".format(n_vocab))\n",
    "print(\"There are {} sequences of {} characters in a row.\".format(n_patterns, seq_length))\n",
    "\n",
    "def train_test(xs, ys, train_percent = .85):\n",
    "    \n",
    "    index = [i for i in range(len(ys))]\n",
    "    random.shuffle(index)\n",
    "    train_index = index[:int(train_percent * len(index))]\n",
    "    test_index = index[int(train_percent * len(index)):]\n",
    "    \n",
    "    x_train = xs[train_index]\n",
    "    x_test = xs[test_index]\n",
    "    y_train = ys[train_index]\n",
    "    y_test = ys[test_index]\n",
    "    \n",
    "    return(x_train, x_test, y_train, y_test)\n",
    "    \n",
    "x_train, x_test, y_train, y_test = train_test(x, y)\n",
    "print(\"\\nTrain: \\n\\t Input: {0}, \\t Output: {1}.\".format(x_train.shape, y_train.shape))\n",
    "print(\"\\nTest: \\n\\t Input: {0}, \\t Output: {1}.\".format(x_test.shape, y_test.shape))\n",
    "\n",
    "def get_batch(test = False, size = 64):\n",
    "    if(test): x, y = x_test, y_test\n",
    "    else:     x, y = x_train, y_train\n",
    "    index = [i for i in range(len(y))]\n",
    "    random.shuffle(index)\n",
    "    index = index[:size]\n",
    "    x, y = x[index], y[index]\n",
    "    return(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f02630",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now make model.\n",
    "\n",
    "hidden_dim = 128\n",
    "\n",
    "U = np.random.uniform(0, 1, (hidden_dim, seq_length))\n",
    "W = np.random.uniform(0, 1, (hidden_dim, hidden_dim))\n",
    "V = np.random.uniform(0, 1, (n_vocab, hidden_dim))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def forward(x):\n",
    "    layers = []\n",
    "    prev_s = np.zeros((hidden_dim, 1))\n",
    "    for t in range(seq_length):\n",
    "        new_input = np.zeros(x.shape)\n",
    "        new_input[t] = x[t]\n",
    "        mulu = np.dot(U, new_input)\n",
    "        mulu = np.expand_dims(mulu[:,np.argmax(new_input[t])], 1) # Is this right?\n",
    "        mulw = np.dot(W, prev_s)\n",
    "        add = mulw + mulu\n",
    "        s = sigmoid(add)\n",
    "        mulv = np.dot(V, s)\n",
    "        layers.append({'s':s, 'prev_s':prev_s})\n",
    "        prev_s = s\n",
    "    return(mulv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e122daf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given:\n",
      "\t' say '\n",
      "\n",
      "predicted:\n",
      "\t' say  | rrrrrrrrrr'\n"
     ]
    }
   ],
   "source": [
    "def try_it(times = 1, length = 10):\n",
    "    index = [i for i in range(x_test.shape[0])]\n",
    "    random.shuffle(index)\n",
    "    index = index[:times]\n",
    "    xs = x_test[index]\n",
    "    \n",
    "    for x in xs:\n",
    "        x_ = np.argmax(x, 1)\n",
    "        x_ = ''.join([int_to_char[x_[i]] for i in range(seq_length)])\n",
    "        pred_list = []\n",
    "        for i in range(length):\n",
    "            pred = forward(x)\n",
    "            pred_list.append(int_to_char[np.argmax(pred)])\n",
    "            pred = np.expand_dims(to_categorical(char_to_int[pred_list[-1]], num_classes = n_vocab), axis = 0)\n",
    "            x = np.vstack([x[1:], pred])\n",
    "        print(\"\\nGiven:\\n\\t'{}'\\n\\npredicted:\\n\\t'{}'\".format(x_, x_ + \" | \" + \"\".join(pred_list)))\n",
    "    \n",
    "try_it()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6046e64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given:\n",
      "\t'dormo | use out of'\n",
      "\n",
      "predicted:\n",
      "\t'dormo | rrrrrrrrrr'\n"
     ]
    }
   ],
   "source": [
    "def try_it_easier(length = 10):\n",
    "    index = [i for i in range(x_keep.shape[0]-(length + seq_length))]\n",
    "    random.shuffle(index)\n",
    "    index = index[0]\n",
    "    pred_list = []\n",
    "    for i in range(length):\n",
    "        seq_in = raw_text[index + i : index + i + seq_length]\n",
    "        seq_in = [char_to_int[char] for char in seq_in]\n",
    "        seq_in = to_categorical(seq_in, n_vocab)\n",
    "        pred = forward(seq_in)\n",
    "        pred_list.append(int_to_char[np.argmax(pred)])\n",
    "    print(\"\\nGiven:\\n\\t'{}'\\n\\npredicted:\\n\\t'{}'\".format(\n",
    "        raw_text[index : index + seq_length] + \" | \" + raw_text[index + seq_length : index + length + seq_length], \n",
    "        raw_text[index : index + seq_length] + \" | \" + \"\".join(pred_list)))\n",
    "\n",
    "try_it_easier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112ec04b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U:\t(128, 5)\t*\tNew Input:\t(5, 38)\t\t=\tmulu:\t(128, 1).\n",
      "W:\t(128, 128)\t*\tprev_s:\t\t(128, 1)\t=\tmulw:\t(128, 1).\n",
      "mulu + mulw\t=\ts:\t(128, 1).\n",
      "mulu: 0.007 min, 0.996 max. \tmulw: 0.0 min, 0.0 max.\n",
      "\n",
      "U:\t(128, 5)\t*\tNew Input:\t(5, 38)\t\t=\tmulu:\t(128, 1).\n",
      "W:\t(128, 128)\t*\tprev_s:\t\t(128, 1)\t=\tmulw:\t(128, 1).\n",
      "mulu + mulw\t=\ts:\t(128, 1).\n",
      "mulu: 0.004 min, 0.984 max. \tmulw: 34.309 min, 44.698 max.\n",
      "\n",
      "U:\t(128, 5)\t*\tNew Input:\t(5, 38)\t\t=\tmulu:\t(128, 1).\n",
      "W:\t(128, 128)\t*\tprev_s:\t\t(128, 1)\t=\tmulw:\t(128, 1).\n",
      "mulu + mulw\t=\ts:\t(128, 1).\n",
      "mulu: 0.024 min, 0.986 max. \tmulw: 54.807 min, 71.483 max.\n",
      "\n",
      "U:\t(128, 5)\t*\tNew Input:\t(5, 38)\t\t=\tmulu:\t(128, 1).\n",
      "W:\t(128, 128)\t*\tprev_s:\t\t(128, 1)\t=\tmulw:\t(128, 1).\n",
      "mulu + mulw\t=\ts:\t(128, 1).\n",
      "mulu: 0.001 min, 0.999 max. \tmulw: 54.807 min, 71.483 max.\n",
      "\n",
      "U:\t(128, 5)\t*\tNew Input:\t(5, 38)\t\t=\tmulu:\t(128, 1).\n",
      "W:\t(128, 128)\t*\tprev_s:\t\t(128, 1)\t=\tmulw:\t(128, 1).\n",
      "mulu + mulw\t=\ts:\t(128, 1).\n",
      "mulu: 0.012 min, 1.0 max. \tmulw: 54.807 min, 71.483 max.\n",
      "\n",
      "V:\t(38, 128)\t*\ts:\t\t(128, 1)\t=\tmulv:\t(38, 1).\ty: (38,).\n",
      "mulv: 58.016 min, 69.343 max.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2046.1533556393242"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loser(test = False, text = False, batch_size = \"all\"):\n",
    "    if(test): X_, Y_ = x_test, y_test \n",
    "    else:     X_, Y_ = x_train, y_train  \n",
    "    if(batch_size != \"all\"):\n",
    "        index = [i for i in range(len(X_))]\n",
    "        random.shuffle(index)\n",
    "        index = index[:batch_size]\n",
    "        X_, Y_ = X_[index], Y_[index]\n",
    "    loss = 0.0\n",
    "    for i in range(X_.shape[0]):\n",
    "        x, y = X_[i], Y_[i]                    \n",
    "        prev_s = np.zeros((hidden_dim, 1))          \n",
    "        for t in range(seq_length):\n",
    "            new_input = np.zeros(x.shape)    \n",
    "            new_input[t] = x[t]\n",
    "            mulu = np.dot(U, new_input)\n",
    "            mulu = np.expand_dims(mulu[:,np.argmax(new_input[t])], 1) # Is this right?\n",
    "            mulw = np.dot(W, prev_s)\n",
    "            add = mulw + mulu\n",
    "            s = sigmoid(add)\n",
    "            if(text):\n",
    "                print(\"\\nU:\\t{}\\t*\\tNew Input:\\t{}\\t\\t=\\tmulu:\\t{}.\".format(U.shape, new_input.shape, mulu.shape))\n",
    "                print(\"W:\\t{}\\t*\\tprev_s:\\t\\t{}\\t=\\tmulw:\\t{}.\".format(W.shape, prev_s.shape, mulw.shape))\n",
    "                print(\"mulu + mulw\\t=\\ts:\\t{}.\".format(s.shape))\n",
    "                print(\"mulu: {} min, {} max. \\tmulw: {} min, {} max.\".format(\n",
    "                    round(mulu.min(), 3), \n",
    "                    round(mulu.max(), 3), \n",
    "                    round(mulw.min(), 3), \n",
    "                    round(mulw.max(), 3)))\n",
    "            prev_s = s\n",
    "        mulv = np.dot(V, s)\n",
    "        if(text):\n",
    "            print(\"\\nV:\\t{}\\t*\\ts:\\t\\t{}\\t=\\tmulv:\\t{}.\\ty: {}.\".format(V.shape, s.shape, mulv.shape, y.shape))\n",
    "            print(\"mulv: {} min, {} max.\".format(round(mulv.min(), 3), round(mulv.max(), 3)))\n",
    "        loss_per_record = (y - mulv.squeeze(1))**2 / 2\n",
    "        loss += loss_per_record\n",
    "    loss = loss / float(y.shape[0])\n",
    "    loss = loss.sum()\n",
    "    return(loss)\n",
    "\n",
    "loser(test = True, text = True, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e659d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 , Loss:  1616.7112975429702 , Val Loss:  285.26411411998384\n",
      "\n",
      "Let's try it!\n",
      "\n",
      "Given:\n",
      "\t' wild'\n",
      "\n",
      "predicted:\n",
      "\t' wild |                                                                                                     '\n",
      "\n",
      "Given:\n",
      "\t'iece  | all round.  'but she must have a prize herself, you know,' said the mouse.  'of course,' the dodo re'\n",
      "\n",
      "predicted:\n",
      "\t'iece  |                                                                                                     '\n",
      "Epoch:  2 , Loss:  1658.220678419812 , Val Loss:  292.64576646044605\n",
      "\n",
      "Let's try it!\n",
      "\n",
      "Given:\n",
      "\t's a s'\n",
      "\n",
      "predicted:\n",
      "\t's a s |                                                                                                     '\n",
      "\n",
      "Given:\n",
      "\t'ure t | o ask the question?' said the lory.  alice replied eagerly, for she was always ready to talk about h'\n",
      "\n",
      "predicted:\n",
      "\t'ure t |                                                                                                     '\n",
      "Epoch:  3 , Loss:  1682.299772099967 , Val Loss:  296.87396785685206\n",
      "\n",
      "Let's try it!\n",
      "\n",
      "Given:\n",
      "\t'ren s'\n",
      "\n",
      "predicted:\n",
      "\t'ren s |                                                                                                     '\n",
      "\n",
      "Given:\n",
      "\t'd the |  duchess; 'and that's a fact.'  alice did not at all like the tone of this remark, and thought it wo'\n",
      "\n",
      "predicted:\n",
      "\t'd the |                                                                                                     '\n",
      "Epoch:  4 , Loss:  1644.3689065764356 , Val Loss:  290.11031420856796\n",
      "\n",
      "Let's try it!\n",
      "\n",
      "Given:\n",
      "\t'that '\n",
      "\n",
      "predicted:\n",
      "\t'that  |                                                                                                     '\n",
      "\n",
      "Given:\n",
      "\t'ots e | very christmas.'  and she went on planning to herself how she would manage it. 'they must go by the '\n",
      "\n",
      "predicted:\n",
      "\t'ots e |                                                                                                     '\n"
     ]
    }
   ],
   "source": [
    "### Train!\n",
    "\n",
    "bptt_truncate = 5\n",
    "min_clip_value = -10\n",
    "max_clip_value = 10\n",
    "learning_rate = .001\n",
    "\n",
    "for epoch in range(25):\n",
    "    # train model\n",
    "    for i in range(x_train.shape[0]):\n",
    "        x, y = x_train[i], y_train[i]\n",
    "\n",
    "        layers = []\n",
    "        prev_s = np.zeros((hidden_dim, 1))\n",
    "        dU = np.zeros(U.shape)\n",
    "        dV = np.zeros(V.shape)\n",
    "        dW = np.zeros(W.shape)\n",
    "\n",
    "        dU_t = np.zeros(U.shape)\n",
    "        dV_t = np.zeros(V.shape)\n",
    "        dW_t = np.zeros(W.shape)\n",
    "\n",
    "        dU_i = np.zeros(U.shape)\n",
    "        dW_i = np.zeros(W.shape)\n",
    "        \n",
    "        # forward pass\n",
    "        for t in range(T):\n",
    "            new_input = np.zeros(x.shape)\n",
    "            new_input[t] = x[t]\n",
    "            mulu = np.dot(U, new_input)\n",
    "            mulu = np.expand_dims(mulu[:,np.argmax(new_input[t])], 1) # Is this right?\n",
    "            mulw = np.dot(W, prev_s)\n",
    "            add = mulw + mulu\n",
    "            s = sigmoid(add)\n",
    "            #print(\"\\nU:\\t{}\\t*\\tNew Input:\\t{}\\t\\t=\\tmulu:\\t{}.\".format(U.shape, new_input.shape, mulu.shape))\n",
    "            #print(\"W:\\t{}\\t*\\tprev_s:\\t\\t{}\\t=\\tmulw:\\t{}.\".format(W.shape, prev_s.shape, mulw.shape))\n",
    "            #print(\"mulu + mulw\\t=\\ts:\\t{}.\".format(s.shape))\n",
    "            #print(\"mulu: {} min, {} max. \\tmulw: {} min, {} max.\".format(\n",
    "            #    round(mulu.min(), 3), \n",
    "            #    round(mulu.max(), 3), \n",
    "            #    round(mulw.min(), 3), \n",
    "            #    round(mulw.max(), 3)))\n",
    "            layers.append({'s':s, 'prev_s':prev_s})\n",
    "            prev_s = s\n",
    "        mulv = np.dot(V, s)\n",
    "        #print(\"\\nV:\\t{}\\t*\\ts:\\t\\t{}\\t=\\tmulv:\\t{}.\\ty: {}.\".format(V.shape, s.shape, mulv.shape, y.shape))\n",
    "        #print(\"mulv: {} min, {} max.\".format(round(mulv.min(), 3), round(mulv.max(), 3)))\n",
    "        # derivative of pred\n",
    "        dmulv = (mulv.squeeze(1) - y)\n",
    "        dmulv = np.expand_dims(dmulv, 1)\n",
    "        #print(dmulv.shape, mulv.shape, y.shape)\n",
    "\n",
    "        # backward pass\n",
    "        for t in range(T):\n",
    "            #print(\"dmulv: {}, other: {}\".format(dmulv.shape, np.transpose(layers[t]['s']).shape))\n",
    "            dV_t = np.dot(dmulv, np.transpose(layers[t]['s']))\n",
    "            dsv = np.dot(np.transpose(V), dmulv)\n",
    "\n",
    "            ds = dsv\n",
    "            dadd = add * (1 - add) * ds\n",
    "\n",
    "            dmulw = dadd * np.ones_like(mulw)\n",
    "\n",
    "            dprev_s = np.dot(np.transpose(W), dmulw)\n",
    "\n",
    "\n",
    "            for i in range(t-1, max(-1, t-bptt_truncate-1), -1):\n",
    "                #print(i)\n",
    "                ds = dsv + dprev_s\n",
    "                dadd = add * (1 - add) * ds\n",
    "\n",
    "                dmulw = dadd * np.ones_like(mulw)\n",
    "                dmulu = dadd * np.ones_like(mulu)\n",
    "                \n",
    "                #print(\"dW_i BEFORE:\", dW_i.shape)\n",
    "\n",
    "                dW_i = np.dot(W, layers[t]['prev_s'])    # dW_i changes shape here!\n",
    "                dprev_s = np.dot(np.transpose(W), dmulw)\n",
    "                \n",
    "                #print(\"dW_i AFTER:\", dW_i.shape)\n",
    "                #print(\"dU_i BEFORE:\", dU_i.shape)\n",
    "\n",
    "                new_input = np.zeros(x.shape)\n",
    "                new_input[t] = x[t]\n",
    "                dU_i = np.dot(U, new_input)              # dU_i changes shape here!\n",
    "                dU_i = np.expand_dims(dU_i[:,np.argmax(new_input[t])], 1) # Is this right?\n",
    "                dx = np.dot(np.transpose(U), dmulu)\n",
    "                \n",
    "                #print(\"dU_i AFTER:\", dU_i.shape)\n",
    "                \n",
    "                dU_t += dU_i\n",
    "                dW_t += dW_i\n",
    "                \n",
    "            dV += dV_t\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "\n",
    "            if dU.max() > max_clip_value:\n",
    "                dU[dU > max_clip_value] = max_clip_value\n",
    "            if dV.max() > max_clip_value:\n",
    "                dV[dV > max_clip_value] = max_clip_value\n",
    "            if dW.max() > max_clip_value:\n",
    "                dW[dW > max_clip_value] = max_clip_value\n",
    "\n",
    "\n",
    "            if dU.min() < min_clip_value:\n",
    "                dU[dU < min_clip_value] = min_clip_value\n",
    "            if dV.min() < min_clip_value:\n",
    "                dV[dV < min_clip_value] = min_clip_value\n",
    "            if dW.min() < min_clip_value:\n",
    "                dW[dW < min_clip_value] = min_clip_value\n",
    "\n",
    "        # update\n",
    "        U -= learning_rate * dU\n",
    "        V -= learning_rate * dV\n",
    "        W -= learning_rate * dW\n",
    "    \n",
    "    loss = loser()\n",
    "    val_loss = loser(True)\n",
    "    print('Epoch: ', epoch + 1, ', Loss: ', loss, ', Val Loss: ', val_loss)\n",
    "    \n",
    "    print(\"\\nLet's try it!\")\n",
    "    try_it(times = 1, length = 100)\n",
    "    try_it_easier(length = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8d06f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef829d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c990a924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752ec50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
